{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3261305a",
   "metadata": {},
   "source": [
    "Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f6441b",
   "metadata": {},
   "source": [
    "The key factor that determines whether an idea can be examined and tested statistically is measurability. The idea must involve quantifiable variables that can be observed, measured, and compared, allowing for statistical analysis.\n",
    "\n",
    "A good null hypothesis is defined by testability and specificity. It must be a clear, precise statement that proposes no effect or no difference, providing a baseline that can be either rejected or not based on the data.\n",
    "\n",
    "The difference between a null hypothesis (H₀) and an alternative hypothesis (H₁) is:\n",
    "\n",
    "The null hypothesis (H₀) assumes no effect or no difference in the population, acting as the default assumption to be tested.\n",
    "The alternative hypothesis (H₁) suggests there is an effect or a difference, representing the claim that researchers are trying to find evidence for.\n",
    "In hypothesis testing, we assess whether the data provide sufficient evidence to reject the null hypothesis in favor of the alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00070cc1",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23274ce",
   "metadata": {},
   "source": [
    "The sentence emphasizes that when we perform a hypothesis test, the result applies to the entire population rather than just the specific sample we collected data from. Let me explain the key terms in simple terms:\n",
    "\n",
    "xi represents individual data points in the sample—like the score of a specific person.\n",
    "(x with a dash on top) is the sample mean, which is the average of all the data points we collected in our sample.\n",
    "\n",
    "μ (mu) is the population mean, which is the true average for the entire population, but it's often unknown.\n",
    "\n",
    "μ0(mu-zero) is the hypothesized population mean in the null hypothesis—what we assume to be true before we collect data.\n",
    "So, when the video says the outcome of the test refers to the population parameter (like μ, the true average), it means that our goal in hypothesis testing is to draw conclusions about the whole population, not just the specific sample we measured. The sample (xˉ ) is used to estimate and test claims about the population (μ)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571dacc1",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23460546",
   "metadata": {},
   "source": [
    "When we calculate a p-value, we \"imagine a world where the null hypothesis is true\" because the null hypothesis (H₀) serves as the baseline assumption. The p-value helps us determine how likely our observed data would be if this baseline assumption were correct. In simple terms, we're asking:\n",
    "\n",
    "\"If there's really no effect or difference (as the null hypothesis suggests), how unusual is the data we got?\"\n",
    "\n",
    "By imagining the null hypothesis is true, we're testing whether the observed results are just a rare coincidence under that assumption, or whether they are so unlikely that we should doubt the null hypothesis and consider that the alternative hypothesis might be true instead. This process helps us make decisions based on probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4badc230",
   "metadata": {},
   "source": [
    "Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c588aa1",
   "metadata": {},
   "source": [
    "A smaller p-value makes the null hypothesis look more \"ridiculous\" because it indicates that the observed data is highly unlikely under the assumption that the null hypothesis is true.\n",
    "\n",
    "\n",
    "The p-value is the probability of getting results as extreme as, or more extreme than, what you observed, assuming the null hypothesis is true.\n",
    "If the p-value is very small, it means that the chance of seeing the data we got (or something more extreme) is very low if the null hypothesis were correct.\n",
    "So, a small p-value suggests that the data is not what we would expect if the null hypothesis were true, making it seem increasingly implausible. As a result, we are more inclined to reject the null hypothesis and consider that the alternative hypothesis (which suggests there is an effect or difference) might be the more reasonable explanation for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b046a976",
   "metadata": {},
   "source": [
    "Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd56335e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "n_trials = 10000  # Number of simulations\n",
    "n_couples = 124  # Number of couples\n",
    "observed_tilts_right = 80  # Number of right-tilt couples\n",
    "\n",
    "# Simulating head tilts assuming the null hypothesis of 50/50\n",
    "simulations = np.random.binomial(n_couples, 0.5, n_trials)\n",
    "\n",
    "# Calculating the p-value\n",
    "p_value = np.mean(simulations >= observed_tilts_right)\n",
    "p_value\n",
    "# Outputs 0.001, therefore we have strong evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1591f51",
   "metadata": {},
   "source": [
    "The simulated p-value is approximately 0.001. This means that if the null hypothesis (that there is no left or right head tilt preference) were true, there is only a 0.1% chance of observing 80 or more couples tilting their heads to the right out of 124.\n",
    "\n",
    "Given the p-value, we have strong evidence against the null hypothesis, as a p-value this small indicates that such an outcome is very unlikely under the assumption of no preference (50/50 head tilt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f66484",
   "metadata": {},
   "source": [
    "Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc8737",
   "metadata": {},
   "source": [
    "\n",
    "No, a smaller p-value cannot definitively prove that the null hypothesis is false. A p-value simply tells us how likely the observed data is under the assumption that the null hypothesis is true. A small p-value suggests that the null hypothesis is unlikely, but it doesn’t provide absolute proof—it only indicates strong evidence against it.\n",
    "\n",
    "Similarly, in the Fido example (from the video), a p-value cannot definitively prove Fido’s innocence or guilt:\n",
    "\n",
    "A small p-value suggests evidence against the assumption of innocence (null hypothesis), but it doesn’t guarantee guilt.\n",
    "A large p-value suggests there isn’t enough evidence to reject the assumption of innocence, but it doesn’t guarantee innocence either.\n",
    "No p-value, no matter how low or high, can definitively prove either hypothesis. In statistics, we deal with probabilities and evidence, not absolute proof."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbfd2cd",
   "metadata": {},
   "source": [
    "Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c139222",
   "metadata": {},
   "source": [
    "1. Interpretation of a One-Sided Test vs. Two-Sided Test\n",
    "Two-sided test: This tests for any difference between the groups (positive or negative). It checks whether the observed statistic is significantly different from the null hypothesis in either direction (larger or smaller).\n",
    "One-sided test: This tests only for a difference in one direction (for example, only whether the treatment group has a greater improvement, or only whether the group shows a worse result).\n",
    "\n",
    "\n",
    "2. Adjusting the Code for a One-Sided Test\n",
    "The current two-sided version checks both directions of the change. To adjust it for a one-sided test, we need to focus on only one direction (e.g., improvement only). The main change will be in how we calculate the p-value after running the simulations.\n",
    "\n",
    "IncreaseProportionSimulations_underH0random[i] = random_improvement.mean()\n",
    "This part simulates proportions assuming the null hypothesis (H₀) is true. The next part would typically calculate the p-value by checking how extreme the observed result is compared to both tails of the null distribution.\n",
    "\n",
    "3. Modification for One-Sided Test\n",
    "Instead of comparing to both tails, we'll only focus on one tail. Specifically:\n",
    "\n",
    "For a one-sided test, where we want to see if there's a significant increase, we only count the proportion of simulations where the random improvement proportions are greater than or equal to the observed statistic.\n",
    "The p-value calculation for the one-sided test would be something like:\n",
    "p_value = np.mean(IncreaseProportionSimulations_underH0random >= observed_proportion)\n",
    "\n",
    "This compares the observed proportion of improvements (from the real data) to only the right tail of the null distribution, testing whether the observed proportion is significantly greater than the random chance improvements under H₀.\n",
    "\n",
    "4. Why the P-value Might Be Smaller in a One-Sided Test\n",
    "In a two-tailed test, we look at both extreme ends (both high and low values), which makes it harder to reject the null hypothesis. A one-sided test, by contrast, focuses on just one extreme, so the p-value is likely to be smaller. The one-sided p-value could be lower because we aren't dividing the probability across both tails—just one tail is considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a36d4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-sided p-value: 0.0565\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "patient_data = pd.DataFrame({\n",
    "    \"PatientID\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"Age\": [45, 34, 29, 52, 37, 41, 33, 48, 26, 39],\n",
    "    \"Gender\": [\"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\"],\n",
    "    \"InitialHealthScore\": [84, 78, 83, 81, 81, 80, 79, 85, 76, 83],\n",
    "    \"FinalHealthScore\": [86, 86, 80, 86, 84, 86, 86, 82, 83, 84]\n",
    "})\n",
    "patient_data\n",
    "# First let's format this data in the manner of last week's HW \"Prelecture\" video\n",
    "# from IPython.display import YouTubeVideo\n",
    "# YouTubeVideo('Xz0x-8-cgaQ', width=800, height=500)  # https://www.youtube.com/watch?v=Xz0x-8-cgaQ\n",
    "\n",
    "patient_data['HealthScoreChange'] = patient_data.FinalHealthScore-patient_data.InitialHealthScore\n",
    "# why do we do the subtraction in this order?\n",
    "patient_data\n",
    "\n",
    "np.random.seed(1)\n",
    "number_of_simulations = 10000\n",
    "n_size = len(patient_data)\n",
    "IncreaseProportionSimulations_underH0random = np.zeros(number_of_simulations)\n",
    "\n",
    "for i in range(number_of_simulations):\n",
    "    random_improvement = np.random.choice([0,1], size=len(patient_data), replace=True)\n",
    "    IncreaseProportionSimulations_underH0random[i] = random_improvement.mean()\n",
    "\n",
    "observed_proportion = (patient_data['HealthScoreChange'] > 0).mean()\n",
    "\n",
    "# For a one-sided test where we're testing for increase\n",
    "p_value = np.mean(IncreaseProportionSimulations_underH0random >= observed_proportion)\n",
    "print(f'One-sided p-value: {p_value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c440a53",
   "metadata": {},
   "source": [
    "Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be70553",
   "metadata": {},
   "source": [
    "Problem Introduction\n",
    "This report examines a hypothesis test inspired by Ronald Fisher’s famous tea experiment. The original experiment tested whether Dr. Muriel Bristol could distinguish between tea prepared with milk first and tea first, by asking her to identify which method was used in 8 blind-tasted cups. Here, we replicate the experiment in the context of STA130 students. We take a sample of 80 students, asking each to taste one cup and determine whether milk or tea was poured first. Out of these 80 students, 49 correctly identified which liquid was poured first. The purpose of this analysis is to determine whether the students' ability to identify correctly could have been due to random guessing, or if the observed outcome suggests evidence that students were able to identify the pouring order more accurately than chance would allow.\n",
    "\n",
    "\n",
    "Relationship Between This Experiment and the Original with Fisher and Bristol\n",
    "The two experiments are similar in that both attempt to determine whether individuals can correctly identify the order of milk and tea based on taste. The main differences are the sample size and the population. Fisher’s experiment involved one person (Bristol), with a small sample size of 8, while the STA130 experiment involves 80 students. While Bristol’s ability could be seen as a personalized skill, the STA130 experiment is more abstract, assessing the general ability of a group of students.\n",
    "\n",
    "\n",
    "Statements of the Null Hypothesis and Alternative Hypothesis\n",
    "Null Hypothesis (H0):\n",
    "The students are guessing randomly, and their probability of correctly identifying whether the milk or tea was poured first is 50%, meaning p=0.5.\n",
    "\n",
    "\n",
    "Informal Interpretation: \n",
    "The students are just guessing, and the observed outcome (49 correct out of 80) could easily happen by chance.\n",
    "Alternative Hypothesis (H1): The students are not guessing, and the probability of correctly identifying which was poured first is greater than 50%, meaning p>0.5.\n",
    "\n",
    "\n",
    "Informal Interpretation: \n",
    "The students are able to identify the order of pouring more accurately than would be expected by random guessing.\n",
    "\n",
    "\n",
    "Quantitative Analysis\n",
    "We will perform a one-sided hypothesis test to assess whether the proportion of students who correctly identified the order of pouring (49 out of 80) is significantly greater than 50%. To do this, we will simulate the null hypothesis by assuming a 50/50 chance of guessing correctly and calculate the p-value to determine whether the observed outcome is unlikely under the null hypothesis.\n",
    "\n",
    "\n",
    "Methodology Code and Explanations\n",
    "We will simulate random guessing under the null hypothesis (students randomly guessing with a probability of 0.5) and compare this to the observed result to compute the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Given data\n",
    "n_students = 80  # number of students\n",
    "correct_guesses = 49  # number of correct guesses\n",
    "p_null = 0.5  # null hypothesis assumes random guessing with p = 0.5\n",
    "n_simulations = 10000  # number of simulations\n",
    "\n",
    "# Simulating random guessing under the null hypothesis\n",
    "simulated_correct_guesses = np.random.binomial(n_students, p_null, n_simulations)\n",
    "\n",
    "# Calculate the proportion of simulations where the number of correct guesses is >= observed correct guesses\n",
    "p_value = np.mean(simulated_correct_guesses >= correct_guesses)\n",
    "p_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b4a419",
   "metadata": {},
   "source": [
    "Findings and Discussion\n",
    "After running the simulation, the p-value will tell us how likely it is to observe 49 or more correct guesses under the assumption that students are just randomly guessing. If the p-value is small (typically less than 0.05), it suggests that the observed result is unlikely under the null hypothesis, and we may reject the null in favor of the alternative hypothesis.\n",
    "\n",
    "Conclusion Regarding the Null Hypothesis\n",
    "If the p-value is small, we would conclude that there is evidence to suggest that the students are not just randomly guessing and that they have some ability to correctly identify the order of pouring. If the p-value is large, we would conclude that the result is consistent with random guessing, and we fail to reject the null hypothesis.\n",
    "\n",
    "This analysis approach provides a clear way to assess whether the observed number of correct guesses is statistically significant, allowing us to draw meaningful conclusions from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d75805",
   "metadata": {},
   "source": [
    "Question 9\n",
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505dc180",
   "metadata": {},
   "source": [
    "Chatbot log summary:\n",
    "\n",
    "\n",
    "Here's a summary of our interaction:\n",
    "\n",
    "Hypothesis Testing Concept: We began discussing the nature of hypothesis testing, focusing on the key difference between testable and untestable ideas in statistics, the importance of specificity in a null hypothesis, and the distinctions between a null and alternative hypothesis in testing.\n",
    "\n",
    "Null Hypothesis and p-values: We explored why p-values are calculated by assuming the null hypothesis is true, and why a smaller p-value suggests that the null hypothesis looks increasingly unlikely, though it can never definitively prove it false.\n",
    "\n",
    "One-sided vs. Two-sided Tests: We then discussed how to adjust code from a previous demo to calculate a p-value for a one-sided test instead of a two-sided one. You asked about the implications of this adjustment and why a one-sided test might result in a smaller p-value.\n",
    "\n",
    "Fisher’s Tea Experiment: We examined a modern version of the famous Fisher’s Tea Experiment, where students were asked to guess whether milk or tea was poured first. We formulated a hypothesis test to analyze whether the number of correct guesses in the student group was significantly greater than random guessing.\n",
    "\n",
    "Quantitative Analysis with Code: We walked through a code simulation of the null hypothesis (random guessing), calculating a p-value to determine whether the observed result was statistically significant. We discussed how the code works, including concepts like simulation, random guessing, and the binomial distribution.\n",
    "\n",
    "Statistical Analysis and Reporting: The report outlined formal and informal interpretations of null and alternative hypotheses, quantitative analysis using simulations, code methodology, and interpretation of results. You asked for a breakdown of how p-values help us make decisions without definitively proving innocence or guilt.\n",
    "\n",
    "Overall, we’ve covered a wide range of statistical concepts, focusing on hypothesis testing, p-values, simulation, and experimental analysis, with a practical approach using Python to simulate and analyze real-world scenarios like Fisher’s Tea Experiment.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61da1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
